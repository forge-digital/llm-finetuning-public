{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook is for testing and getting familiar with embedding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a sentence transformer embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = SentenceTransformer(\"msmarco-distilbert-base-v4\")\n",
    "embedder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the above embedder has first a transformer layer and then a pooling layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try out the embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_text = \"The European Union (EU) is a supranational political and economic union of 27 member states that are located primarily in Europe\"\n",
    "embeddings = embedder.encode(wikipedia_text)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two different texts can be compared by using different metrics. Cosine similarity (inner product) is one popular metric. Try that out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_text_2 = \"The EU has often been described as a sui generis political entity (without precedent or comparison) combining the characteristics of both a federation and a confederation.\"\n",
    "embeddings_2 = embedder.encode(wikipedia_text_2)\n",
    "\n",
    "# Calculate the similarity between the two embeddings\n",
    "def cosine(u, v):\n",
    "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))\n",
    "\n",
    "cosine(embeddings, embeddings_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_about_python = \"Python is an interpreted, high-level and general-purpose programming language. Python's design philosophy emphasizes code readability with its notable use of significant indentation.\"\n",
    "embeddings_python = embedder.encode(text_about_python)\n",
    "\n",
    "cosine(embeddings, embeddings_python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "\n",
    "Calculate the similarity of some other text samples. Are the similarities intuitive for you?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the similary by constructing the pooling \"by hand\"\n",
    "\n",
    "This example is exactly the same as in Hugging Face model card.\n",
    "* Tokenizer and pre-trained model are loaded\n",
    "* Input text is tokenized\n",
    "* Tokenized input is fed into the model\n",
    "* Model output (hidden output) is pooled with `mean_pooling` function to produce the embeddings.\n",
    "\n",
    "This is how the above SentenceTransformer works behind the scenes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_embeddings(sentences, model_name):\n",
    "\n",
    "    #Mean Pooling - Take attention mask into account for correct averaging\n",
    "    def mean_pooling(model_output, attention_mask):\n",
    "        token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "    # Load model from HuggingFace Hub\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    # Tokenize sentences\n",
    "    encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "\n",
    "    # Perform pooling. In this case, max pooling.\n",
    "    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "    return sentence_embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate first the similary between the sentences by using the msmarco-distilbert-base-v4 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings = create_embeddings([wikipedia_text, wikipedia_text_2, text_about_python], 'sentence-transformers/msmarco-distilbert-base-v4')\n",
    "\n",
    "print(\"Similarity between wiki sentences:\", cosine(sentence_embeddings[0], sentence_embeddings[1]))\n",
    "print(\"Similarity between wiki and python sentences:\", cosine(sentence_embeddings[0], sentence_embeddings[2]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the same thing with the distilbert base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings = create_embeddings([wikipedia_text, wikipedia_text_2, text_about_python], 'distilbert/distilbert-base-uncased')\n",
    "\n",
    "print(\"Similarity between wiki sentences:\", cosine(sentence_embeddings[0], sentence_embeddings[1]))\n",
    "print(\"Similarity between wiki and python sentences:\", cosine(sentence_embeddings[0], sentence_embeddings[2]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "\n",
    "Could you use the untrained embedding model created from `distilbert/distilbert-base-uncased`for creating embeddings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO (You can continue also with these if you have time)\n",
    "\n",
    "Advanced: Evaluate the model accucary by using for instance the dataset `mteb/stsbenchmark-sts`.\n",
    "\n",
    "Advanced: Fine-tune your embedding model with the same dataset and see if the accuracy imporoved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
