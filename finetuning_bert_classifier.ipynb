{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import DistilBertTokenizerFast, DistilBertModel\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "import gc\n",
    "from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertForMaskedLM, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning example notebook\n",
    "\n",
    "Below code is an example of how you can fine-tune an existing model. There are some TODO items for you to explore different things, but as said, do not feel restricted by these but instead try to think what is still unknown for you or what could you try out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download data\n",
    "\n",
    "Use Imdb dataset for this exercise. The dataset contains movie reviews and review sentiment (positive or negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Imdb dataset from Hugging Face\n",
    "dataset = load_dataset(\"stanfordnlp/imdb\")\n",
    "# Make sure the data is shuffled\n",
    "dataset = dataset.shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset contains train and test splits and text, label pairs where label is 0 or 1 depending on sentiment\n",
    "print(dataset[\"train\"][0].keys())\n",
    "print(dataset[\"train\"][0][\"text\"])\n",
    "print(dataset[\"train\"][0][\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch tokenizer and encode the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='distilbert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    tokenizer.truncation_side = \"left\"\n",
    "    return tokenizer(\n",
    "        text = examples[\"text\"],\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_encoded = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "What did the tokenizer function add to the dataset and how does it look now?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch the model to be fine-tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the base model\n",
    "base_model = DistilBertModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the base model\n",
    "test_base_model_input = \"Test\"\n",
    "encoded_input = tokenizer(test_base_model_input, return_tensors=\"pt\")\n",
    "output = base_model(**encoded_input)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, the base model does not have a head, so fetch a base model with sequence classification head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_classification = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "What is the difference between the base_model and base_model_classification?\n",
    "\n",
    "Can we use the base model with classification head directly for classification? Why? Try it out.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune model\n",
    "Test first with smaller number of samples (num_train_spochs = 3, number of train samples = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train data into train and eval datasets. Dataset has a method for this built in.\n",
    "dataset_train_eval = dataset_encoded[\"train\"].train_test_split(test_size=0.2)\n",
    "dataset_train_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_train_samples = 500\n",
    "n_eval_samples = 100\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./distilbert_classification_results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=50,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./distilbert_classification_logs',\n",
    "    logging_steps=10,\n",
    "    #use_mps_device=True\n",
    ")\n",
    "\n",
    "fine_tuned_model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=fine_tuned_model,\n",
    "    args=training_args,\n",
    "    train_dataset=Dataset.from_dict(dataset_train_eval[\"train\"][:n_train_samples]),\n",
    "    eval_dataset=Dataset.from_dict(dataset_train_eval[\"test\"][:n_eval_samples])\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "Test the fine-tuned model with some test reviews.\n",
    "\n",
    "Hint: If you encounter an error regarding memory not allocated in some device, allocate the tokens in the proper device first.\n",
    "Example:\n",
    "\n",
    "`tokenizer(test_review, return_tensors=\"pt).to(\"mps\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "Is the true base model also changed when fine-tuning as above? Can you check that somehow?\n",
    "\n",
    "Hint: Check if the models weights are the same:\n",
    "\n",
    "`base_model_classification._modules[\"embeddings\"].word_embeddings.weight`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is a method for doing inference in batches, and calculating the model accuracy.\n",
    "def inference(model, dataset_encoded_test, batch_size: int, n_samples: int, device: str=\"mps\"):\n",
    "    print(f\"Inference on {device}...\")\n",
    "    model.to(device)\n",
    "\n",
    "    for i in range(0,n_samples//batch_size):\n",
    "        print(\"Batch: \", i)\n",
    "        input_ids = torch.LongTensor(dataset_encoded_test[\"input_ids\"][i*batch_size:(i+1)*batch_size]).to(device)\n",
    "        attention_mask = torch.LongTensor(dataset_encoded_test[\"attention_mask\"][i*batch_size:(i+1)*batch_size]).to(device)\n",
    "\n",
    "        with torch.no_grad():   \n",
    "            logits = model(input_ids = input_ids,\n",
    "                        attention_mask = attention_mask).logits\n",
    "        if i == 0:\n",
    "            logits_all = logits\n",
    "        else:\n",
    "            logits_all = torch.cat((logits_all, logits),0)\n",
    "\n",
    "    if device == \"cpu\":\n",
    "        predicted_labels = np.argmax(logits_all.detach().numpy(), axis=1)\n",
    "    else:\n",
    "        predicted_labels = np.argmax(logits_all.cpu().detach().numpy(), axis=1)\n",
    "    test_set_labels = dataset_encoded_test[\"label\"][:(n_samples//batch_size*batch_size)]\n",
    "    \n",
    "    print(predicted_labels[:20])\n",
    "    print(test_set_labels[:20])\n",
    "    print(f\"Model accuracy is: {accuracy_score(test_set_labels, predicted_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference(model=fine_tuned_model, dataset_encoded_test=dataset_encoded[\"test\"], batch_size=200, n_samples=1000, device=\"mps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Freezing layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_freezed = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "for name, param in model_freezed.named_parameters():\n",
    "     if \"distilbert.\" in name:\n",
    "        param.requires_grad = False\n",
    "\n",
    "trainer_freezed = Trainer(\n",
    "    model=model_freezed,\n",
    "    args=training_args,\n",
    "    train_dataset=Dataset.from_dict(dataset_train_eval[\"train\"][:n_train_samples]),\n",
    "    eval_dataset=Dataset.from_dict(dataset_train_eval[\"test\"][:n_eval_samples])\n",
    ")\n",
    "trainer_freezed.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference(model=model_freezed, dataset_encoded_test=dataset_encoded[\"test\"], batch_size=200, n_samples=1000, device=\"mps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PEFT training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same base model for LoRA training\n",
    "model_lora_base = DistilBertForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "lora_config = LoraConfig(target_modules = ['word_embeddings', 'q_lin', 'k_lin', 'v_lin', 'out_lin','pre_classifier','classifier'])\n",
    "model_lora = get_peft_model(model_lora_base, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_lora= Trainer(\n",
    "    model=model_lora,\n",
    "    args=training_args,\n",
    "    train_dataset=Dataset.from_dict(dataset_train_eval[\"train\"][:n_train_samples]),\n",
    "    eval_dataset=Dataset.from_dict(dataset_train_eval[\"test\"][:n_eval_samples])\n",
    ")\n",
    "trainer_lora.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference(model=model_lora, dataset_encoded_test=dataset_encoded[\"test\"], batch_size=200, n_samples=1000, device=\"mps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "Here are some questions and topics to explore.\n",
    "\n",
    "1. What do you notice about the accuracy and training time with full fine-tuning, with freezing layers or with LoRa fine-tuning? Can you find some explanations for your findings?\n",
    "2. Test different LoRa config parameters. How does the rank (parameter r) affect the number of trainable parameters. Hint: `print_trainable_parameters`?\n",
    "3. How is the model accuracy affected when using a lower rank for instance?\n",
    "4. For further study: Following the above principles, try fine-tuning a classifier with some different data that you can find from Hugging Face. Or try to fine tune these models further with some other sentiment data (other than imbd reviews).\n",
    "5. Explore how the model accuracy improves as you use more data for training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
